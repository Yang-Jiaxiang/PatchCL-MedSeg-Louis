{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c06effd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/tf/PatchCL-MedSeg-jiyu'\n",
    "dataset_path = '/tf/dataset/0_data_dataset_voc_950_kidney'\n",
    "output_dir = base_path + '/dataset/splits/kidney'\n",
    "\n",
    "supervised_loss_path = base_path + '/supervised pre training_loss.csv'\n",
    "SSL_loss_path = base_path + '/SSL_loss.csv'\n",
    "\n",
    "voc_mask_color_map = [\n",
    "    [0, 0, 0], #_background\n",
    "    [128, 0, 0] #kidney\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41fe5a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "sys.path.append(base_path)\n",
    "\n",
    "from utils.transform import Transform\n",
    "from utils.stochastic_approx import StochasticApprox\n",
    "from utils.model import Network\n",
    "from utils.datasets_PASCAL import PascalVOCDataset\n",
    "from utils.queues import Embedding_Queues\n",
    "from utils.CELOSS import CE_loss\n",
    "from utils.patch_utils import _get_patches\n",
    "from utils.aug_utils import batch_augment\n",
    "from utils.get_embds import get_embeddings\n",
    "from utils.const_reg import consistency_cost\n",
    "from utils.plg_loss import PCGJCL\n",
    "from utils.torch_poly_lr_decay import PolynomialLRDecay\n",
    "from utils.loss_file import save_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6362262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_batch_size = 32\n",
    "img_size = 512\n",
    "batch_size = 4\n",
    "num_classes = 2\n",
    "transform = Transform(img_size, num_classes)\n",
    "embd_queues = Embedding_Queues(num_classes)\n",
    "stochastic_approx = StochasticApprox(num_classes,0.5,0.8)\n",
    "dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0426150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "teacher_model = Network()\n",
    "\n",
    "#Turning off gradients for teacher model\n",
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad=False\n",
    "    #Esuring mothe the models have same weight\n",
    "teacher_model.load_state_dict(model.state_dict())\n",
    "model.contrast=False\n",
    "teacher_model.contrast = False\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(dev)\n",
    "teacher_model = nn.DataParallel(teacher_model)\n",
    "teacher_model=teacher_model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8109bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss=CE_loss(num_classes, image_size=img_size)\n",
    "metrics=[smp.utils.metrics.IoU(threshold=0.5)]\n",
    "\n",
    "optimizer_pretrain=torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "optimizer_ssl=torch.optim.SGD(model.parameters(),lr=0.007)\n",
    "scheduler = PolynomialLRDecay(optimizer=optimizer_pretrain, max_decay_steps=200, end_learning_rate=0.0001, power=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af9a1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labeled_dataset:  285\n",
      "number of unlabeled_dataset:  570\n",
      "number of val_dataset:  95\n"
     ]
    }
   ],
   "source": [
    "labeled_dataset = PascalVOCDataset(txt_file=output_dir + \"/1-3/labeled.txt\", image_size=img_size, root_dir=dataset_path, labeled=True, colormap=voc_mask_color_map)\n",
    "unlabeled_dataset = PascalVOCDataset(txt_file=output_dir + \"/1-3/unlabeled.txt\", image_size=img_size, root_dir=dataset_path, labeled=False, colormap=voc_mask_color_map)\n",
    "val_dataset = PascalVOCDataset(txt_file=output_dir + \"/val.txt\", image_size=img_size, root_dir=dataset_path, labeled=True)\n",
    "\n",
    "labeled_loader = DataLoader(labeled_dataset, batch_size=batch_size)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "print('number of labeled_dataset: ', len(labeled_dataset))\n",
    "print('number of unlabeled_dataset: ', len(unlabeled_dataset))\n",
    "print('number of val_dataset: ', len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "600fa4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 256, 256]) torch.Size([8, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# 測試數據加載器\n",
    "for images, masks in labeled_loader:\n",
    "    print(images.shape, masks.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdf2e009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel 0 pixel counts: {0.0: 59524, 1.0: 6012}\n",
      "Channel 1 pixel counts: {0.0: 6012, 1.0: 59524}\n",
      "Original masks shape:  torch.Size([8, 256, 256])\n",
      "Multi-channel masks shape:  torch.Size([8, 2, 256, 256])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 創建一個新的形狀為 [8, 2, 256, 256] 的張量\n",
    "multi_channel_masks = torch.zeros((masks.size(0), 2, masks.size(1), masks.size(2)), dtype=torch.float32)\n",
    "\n",
    "# 將單通道遮罩轉換為多通道遮罩\n",
    "multi_channel_masks[:, 0, :, :] = masks.float()  # 原始遮罩\n",
    "multi_channel_masks[:, 1, :, :] = 1 - masks.float()  # 反轉遮罩\n",
    "\n",
    "# 檢查一個樣本中的唯一值和像素計數\n",
    "imgs_np = multi_channel_masks[0, 0].numpy()\n",
    "unique, counts = np.unique(imgs_np, return_counts=True)\n",
    "pixel_counts = dict(zip(unique, counts))\n",
    "print(\"Channel 0 pixel counts:\", pixel_counts)\n",
    "\n",
    "imgs_np = multi_channel_masks[0, 1].numpy()\n",
    "unique, counts = np.unique(imgs_np, return_counts=True)\n",
    "pixel_counts = dict(zip(unique, counts))\n",
    "print(\"Channel 1 pixel counts:\", pixel_counts)\n",
    "print(\"Original masks shape: \", masks.shape)\n",
    "print(\"Multi-channel masks shape: \", multi_channel_masks.shape)\n",
    "\n",
    "# 顯示原圖和多通道遮罩\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(masks[0].numpy(), cmap='gray')\n",
    "plt.title(\"Original Mask\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(multi_channel_masks[0, 0].numpy(), cmap='gray')\n",
    "plt.title(\"Channel 0 Mask\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(multi_channel_masks[0, 1].numpy(), cmap='gray')\n",
    "plt.title(\"Channel 1 Mask (Inverted)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b31db34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "#Send psudo masks & imgs to cpu\n",
    "p_masks = multi_channel_masks\n",
    "imgs = images\n",
    "\n",
    "print(imgs.shape)\n",
    "\n",
    "#get classwise patch list\n",
    "patch_list = _get_patches(imgs, p_masks,classes=num_classes,background=True,img_size=img_size,patch_size=contrastive_batch_size)\n",
    "\n",
    "#stochastic approximation filtering and threshold update\n",
    "#qualified_patch_list = stochastic_approx.update(patch_list)\n",
    "qualified_patch_list = patch_list\n",
    "\n",
    "#make augmentations for teacher model\n",
    "augmented_patch_list = batch_augment(qualified_patch_list,contrastive_batch_size)\n",
    "\n",
    "\n",
    "#convert to tensor\n",
    "aug_tensor_patch_list=[]\n",
    "qualified_tensor_patch_list=[]\n",
    "for i in range(len(augmented_patch_list)):\n",
    "    if augmented_patch_list[i] is not None:\n",
    "        aug_tensor_patch_list.append(torch.tensor(augmented_patch_list[i]))\n",
    "        qualified_tensor_patch_list.append(torch.tensor(qualified_patch_list[i]))\n",
    "    else:\n",
    "        aug_tensor_patch_list.append(None)\n",
    "        qualified_tensor_patch_list.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "352c9fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
      "  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU \u0001 has a total capacity of 23.69 GiB of which 29.94 MiB is free. Process 3578800 has 23.65 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 257.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Exception raised from malloc at ../c10/cuda/CUDACachingAllocator.cpp:1143 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x71511046d897 in /usr/local/lib/python3.8/dist-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x35abf (0x715110528abf in /usr/local/lib/python3.8/dist-packages/torch/lib/libc10_cuda.so)\n",
      "frame #2: <unknown function> + 0x35ca7 (0x715110528ca7 in /usr/local/lib/python3.8/dist-packages/torch/lib/libc10_cuda.so)\n",
      "frame #3: <unknown function> + 0x360e7 (0x7151105290e7 in /usr/local/lib/python3.8/dist-packages/torch/lib/libc10_cuda.so)\n",
      "frame #4: <unknown function> + 0x108fdf2 (0x7150a5862df2 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #5: <unknown function> + 0x1095d83 (0x7150a5868d83 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0x1097c2c (0x7150a586ac2c in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #7: <unknown function> + 0x109817b (0x7150a586b17b in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #8: <unknown function> + 0x107aca2 (0x7150a584dca2 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #9: at::native::cudnn_convolution(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool) + 0x53f (0x7150a584e66f in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #10: <unknown function> + 0x32d0a9e (0x7150a7aa3a9e in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #11: <unknown function> + 0x32e8251 (0x7150a7abb251 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #12: at::_ops::cudnn_convolution::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool) + 0x2bb (0x7150db0bdc2b in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool) + 0x13cb (0x7150da2f880b in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: <unknown function> + 0x2e0089f (0x7150db48689f in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: <unknown function> + 0x2e071fc (0x7150db48d1fc in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #16: at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool) + 0x344 (0x7150dabcf6f4 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #17: at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long) + 0x3b8 (0x7150da2ebe88 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #18: <unknown function> + 0x2e0013c (0x7150db48613c in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #19: <unknown function> + 0x2e07068 (0x7150db48d068 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #20: at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt) + 0x17b (0x7150dab8d38b in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #21: <unknown function> + 0x4503901 (0x7150dcb89901 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #22: <unknown function> + 0x4504879 (0x7150dcb8a879 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #23: at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt) + 0x2d4 (0x7150dabce4f4 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #24: <unknown function> + 0x19bd900 (0x7150da043900 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #25: at::native::conv2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt) + 0x16b (0x7150da2ef76b in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #26: <unknown function> + 0x2ff96c3 (0x7150db67f6c3 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #27: <unknown function> + 0x2ff995d (0x7150db67f95d in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #28: at::_ops::conv2d::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt) + 0x26e (0x7150db1f295e in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #29: <unknown function> + 0x68474d (0x7150fab7f74d in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_python.so)\n",
      "frame #30: PyCFunction_Call + 0x59 (0x5f6939 in /usr/bin/python3)\n",
      "frame #31: _PyObject_MakeTpCall + 0x296 (0x5f7506 in /usr/bin/python3)\n",
      "frame #32: _PyEval_EvalFrameDefault + 0x5dce (0x570b8e in /usr/bin/python3)\n",
      "frame #33: /usr/bin/python3() [0x50b07e]\n",
      "frame #34: _PyEval_EvalFrameDefault + 0x5796 (0x570556 in /usr/bin/python3)\n",
      "frame #35: _PyFunction_Vectorcall + 0x1b6 (0x5f6ce6 in /usr/bin/python3)\n",
      "frame #36: /usr/bin/python3() [0x50b17c]\n",
      "frame #37: PyObject_Call + 0x62 (0x5f60b2 in /usr/bin/python3)\n",
      "frame #38: _PyEval_EvalFrameDefault + 0x1f3c (0x56ccfc in /usr/bin/python3)\n",
      "frame #39: _PyEval_EvalCodeWithName + 0x26a (0x5697da in /usr/bin/python3)\n",
      "frame #40: _PyFunction_Vectorcall + 0x393 (0x5f6ec3 in /usr/bin/python3)\n",
      "frame #41: /usr/bin/python3() [0x50b17c]\n",
      "frame #42: PyObject_Call + 0x62 (0x5f60b2 in /usr/bin/python3)\n",
      "frame #43: _PyEval_EvalFrameDefault + 0x1f3c (0x56ccfc in /usr/bin/python3)\n",
      "frame #44: _PyEval_EvalCodeWithName + 0x26a (0x5697da in /usr/bin/python3)\n",
      "frame #45: _PyFunction_Vectorcall + 0x393 (0x5f6ec3 in /usr/bin/python3)\n",
      "frame #46: /usr/bin/python3() [0x59d21e]\n",
      "frame #47: _PyObject_MakeTpCall + 0x296 (0x5f7506 in /usr/bin/python3)\n",
      "frame #48: _PyEval_EvalFrameDefault + 0x5dce (0x570b8e in /usr/bin/python3)\n",
      "frame #49: _PyFunction_Vectorcall + 0x1b6 (0x5f6ce6 in /usr/bin/python3)\n",
      "frame #50: /usr/bin/python3() [0x50b17c]\n",
      "frame #51: PyObject_Call + 0x62 (0x5f60b2 in /usr/bin/python3)\n",
      "frame #52: _PyEval_EvalFrameDefault + 0x1f3c (0x56ccfc in /usr/bin/python3)\n",
      "frame #53: _PyEval_EvalCodeWithName + 0x26a (0x5697da in /usr/bin/python3)\n",
      "frame #54: _PyFunction_Vectorcall + 0x393 (0x5f6ec3 in /usr/bin/python3)\n",
      "frame #55: /usr/bin/python3() [0x50b17c]\n",
      "frame #56: PyObject_Call + 0x62 (0x5f60b2 in /usr/bin/python3)\n",
      "frame #57: _PyEval_EvalFrameDefault + 0x1f3c (0x56ccfc in /usr/bin/python3)\n",
      "frame #58: _PyEval_EvalCodeWithName + 0x26a (0x5697da in /usr/bin/python3)\n",
      "frame #59: _PyFunction_Vectorcall + 0x393 (0x5f6ec3 in /usr/bin/python3)\n",
      "frame #60: /usr/bin/python3() [0x59d21e]\n",
      "frame #61: _PyObject_MakeTpCall + 0x296 (0x5f7506 in /usr/bin/python3)\n",
      "frame #62: _PyEval_EvalFrameDefault + 0x59c7 (0x570787 in /usr/bin/python3)\n",
      "frame #63: _PyFunction_Vectorcall + 0x1b6 (0x5f6ce6 in /usr/bin/python3)\n",
      " (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU \u0001 has a total capacity of 23.69 GiB of which 29.94 MiB is free. Process 3578800 has 23.65 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 257.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Exception raised from malloc at ../c10/cuda/CUDACachingAllocator.cpp:1143 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x71511046d897 in /usr/local/lib/python3.8/dist-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x35abf (0x715110528abf in /usr/local/lib/python3.8/dist-packages/torch/lib/libc10_cuda.so)\n",
      "frame #2: <unknown function> + 0x35ca7 (0x715110528ca7 in /usr/local/lib/python3.8/dist-packages/torch/lib/libc10_cuda.so)\n",
      "frame #3: <unknown function> + 0x360e7 (0x7151105290e7 in /usr/local/lib/python3.8/dist-packages/torch/lib/libc10_cuda.so)\n",
      "frame #4: <unknown function> + 0x108fdf2 (0x7150a5862df2 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #5: <unknown function> + 0x1095d83 (0x7150a5868d83 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0x1097c2c (0x7150a586ac2c in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #7: <unknown function> + 0x109817b (0x7150a586b17b in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #8: <unknown function> + 0x107aca2 (0x7150a584dca2 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #9: at::native::cudnn_convolution(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool) + 0x53f (0x7150a584e66f in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #10: <unknown function> + 0x32d0a9e (0x7150a7aa3a9e in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #11: <unknown function> + 0x32e8251 (0x7150a7abb251 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #12: at::_ops::cudnn_convolution::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool) + 0x2bb (0x7150db0bdc2b in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool) + 0x13cb (0x7150da2f880b in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: <unknown function> + 0x2e0089f (0x7150db48689f in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: <unknown function> + 0x2e071fc (0x7150db48d1fc in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #16: at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool) + 0x344 (0x7150dabcf6f4 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #17: at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long) + 0x3b8 (0x7150da2ebe88 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #18: <unknown function> + 0x2e0013c (0x7150db48613c in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #19: <unknown function> + 0x2e07068 (0x7150db48d068 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #20: at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt) + 0x17b (0x7150dab8d38b in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #21: <unknown function> + 0x4503901 (0x7150dcb89901 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #22: <unknown function> + 0x4504879 (0x7150dcb8a879 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #23: at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt) + 0x2d4 (0x7150dabce4f4 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #24: <unknown function> + 0x19bd900 (0x7150da043900 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #25: at::native::conv2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt) + 0x16b (0x7150da2ef76b in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #26: <unknown function> + 0x2ff96c3 (0x7150db67f6c3 in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #27: <unknown function> + 0x2ff995d (0x7150db67f95d in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #28: at::_ops::conv2d::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt) + 0x26e (0x7150db1f295e in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #29: <unknown function> + 0x68474d (0x7150fab7f74d in /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch_python.so)\n",
      "frame #30: PyCFunction_Call + 0x59 (0x5f6939 in /usr/bin/python3)\n",
      "frame #31: _PyObject_MakeTpCall + 0x296 (0x5f7506 in /usr/bin/python3)\n",
      "frame #32: _PyEval_EvalFrameDefault + 0x5dce (0x570b8e in /usr/bin/python3)\n",
      "frame #33: /usr/bin/python3() [0x50b07e]\n",
      "frame #34: _PyEval_EvalFrameDefault + 0x5796 (0x570556 in /usr/bin/python3)\n",
      "frame #35: _PyFunction_Vectorcall + 0x1b6 (0x5f6ce6 in /usr/bin/python3)\n",
      "frame #36: /usr/bin/python3() [0x50b17c]\n",
      "frame #37: PyObject_Call + 0x62 (0x5f60b2 in /usr/bin/python3)\n",
      "frame #38: _PyEval_EvalFrameDefault + 0x1f3c (0x56ccfc in /usr/bin/python3)\n",
      "frame #39: _PyEval_EvalCodeWithName + 0x26a (0x5697da in /usr/bin/python3)\n",
      "frame #40: _PyFunction_Vectorcall + 0x393 (0x5f6ec3 in /usr/bin/python3)\n",
      "frame #41: /usr/bin/python3() [0x50b17c]\n",
      "frame #42: PyObject_Call + 0x62 (0x5f60b2 in /usr/bin/python3)\n",
      "frame #43: _PyEval_EvalFrameDefault + 0x1f3c (0x56ccfc in /usr/bin/python3)\n",
      "frame #44: _PyEval_EvalCodeWithName + 0x26a (0x5697da in /usr/bin/python3)\n",
      "frame #45: _PyFunction_Vectorcall + 0x393 (0x5f6ec3 in /usr/bin/python3)\n",
      "frame #46: /usr/bin/python3() [0x59d21e]\n",
      "frame #47: _PyObject_MakeTpCall + 0x296 (0x5f7506 in /usr/bin/python3)\n",
      "frame #48: _PyEval_EvalFrameDefault + 0x5dce (0x570b8e in /usr/bin/python3)\n",
      "frame #49: _PyFunction_Vectorcall + 0x1b6 (0x5f6ce6 in /usr/bin/python3)\n",
      "frame #50: /usr/bin/python3() [0x50b17c]\n",
      "frame #51: PyObject_Call + 0x62 (0x5f60b2 in /usr/bin/python3)\n",
      "frame #52: _PyEval_EvalFrameDefault + 0x1f3c (0x56ccfc in /usr/bin/python3)\n",
      "frame #53: _PyEval_EvalCodeWithName + 0x26a (0x5697da in /usr/bin/python3)\n",
      "frame #54: _PyFunction_Vectorcall + 0x393 (0x5f6ec3 in /usr/bin/python3)\n",
      "frame #55: /usr/bin/python3() [0x50b17c]\n",
      "frame #56: PyObject_Call + 0x62 (0x5f60b2 in /usr/bin/python3)\n",
      "frame #57: _PyEval_EvalFrameDefault + 0x1f3c (0x56ccfc in /usr/bin/python3)\n",
      "frame #58: _PyEval_EvalCodeWithName + 0x26a (0x5697da in /usr/bin/python3)\n",
      "frame #59: _PyFunction_Vectorcall + 0x393 (0x5f6ec3 in /usr/bin/python3)\n",
      "frame #60: /usr/bin/python3() [0x59d21e]\n",
      "frame #61: _PyObject_MakeTpCall + 0x296 (0x5f7506 in /usr/bin/python3)\n",
      "frame #62: _PyEval_EvalFrameDefault + 0x59c7 (0x570787 in /usr/bin/python3)\n",
      "frame #63: _PyFunction_Vectorcall + 0x1b6 (0x5f6ce6 in /usr/bin/python3)\n",
      " (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB. GPU \u0001 has a total capacity of 23.69 GiB of which 7.94 MiB is free. Process 3578800 has 23.67 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 267.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mcontrast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m student_emb_list \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mqualified_tensor_patch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudent_emb_list len: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(student_emb_list))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#get embeddings of augmented patches through teacher model\u001b[39;00m\n",
      "File \u001b[0;32m/tf/PatchCL-MedSeg-jiyu/utils/get_embds.py:17\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(model, patch_list, studentBool, batch_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m studentBool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mto(dev)\n\u001b[0;32m---> 17\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m emb\u001b[38;5;241m=\u001b[39memb\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print('emb',emb.shape)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/data_parallel.py:184\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/data_parallel.py:189\u001b[0m, in \u001b[0;36mDataParallel.replicate\u001b[0;34m(self, module, device_ids)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplicate\u001b[39m(\u001b[38;5;28mself\u001b[39m, module: T, device_ids: Sequence[Union[\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T]:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/replicate.py:110\u001b[0m, in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m    108\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(network\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m    109\u001b[0m param_indices \u001b[38;5;241m=\u001b[39m {param: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(params)}\n\u001b[0;32m--> 110\u001b[0m param_copies \u001b[38;5;241m=\u001b[39m \u001b[43m_broadcast_coalesced_reshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m buffers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(network\u001b[38;5;241m.\u001b[39mbuffers())\n\u001b[1;32m    113\u001b[0m buffers_rg: List[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/replicate.py:83\u001b[0m, in \u001b[0;36m_broadcast_coalesced_reshape\u001b[0;34m(tensors, devices, detach)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# Use the autograd function to broadcast if not detach\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensors) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 83\u001b[0m         tensor_copies \u001b[38;5;241m=\u001b[39m \u001b[43mBroadcast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [tensor_copies[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(tensors)]\n\u001b[1;32m     85\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tensor_copies), \u001b[38;5;28mlen\u001b[39m(tensors))]\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:23\u001b[0m, in \u001b[0;36mBroadcast.forward\u001b[0;34m(ctx, target_gpus, *inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m ctx\u001b[38;5;241m.\u001b[39mnum_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m     22\u001b[0m ctx\u001b[38;5;241m.\u001b[39minput_device \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_device()\n\u001b[0;32m---> 23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_coalesced\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m non_differentiables \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, input_requires_grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ctx\u001b[38;5;241m.\u001b[39mneeds_input_grad[\u001b[38;5;241m1\u001b[39m:]):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/comm.py:57\u001b[0m, in \u001b[0;36mbroadcast_coalesced\u001b[0;34m(tensors, devices, buffer_size)\u001b[0m\n\u001b[1;32m     55\u001b[0m devices \u001b[38;5;241m=\u001b[39m [_get_device_index(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devices]\n\u001b[1;32m     56\u001b[0m tensors \u001b[38;5;241m=\u001b[39m [_handle_complex(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_broadcast_coalesced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU \u0001 has a total capacity of 23.69 GiB of which 7.94 MiB is free. Process 3578800 has 23.67 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 267.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "#get embeddings of qualified patches through student model\n",
    "model=model.train()\n",
    "model.module.contrast=True\n",
    "student_emb_list = get_embeddings(model,qualified_tensor_patch_list,True)\n",
    "print('student_emb_list len: ', len(student_emb_list))\n",
    "\n",
    "\n",
    "#get embeddings of augmented patches through teacher model\n",
    "teacher_model.train()\n",
    "teacher_model.module.contrast = True\n",
    "teacher_embedding_list = get_embeddings(teacher_model,aug_tensor_patch_list,False)\n",
    "print('teacher_embedding_list len: ', len(teacher_embedding_list))\n",
    "\n",
    "#enqueue these\n",
    "embd_queues.enqueue(teacher_embedding_list)\n",
    "\n",
    "#calculate PCGJCL loss\n",
    "PCGJCL_loss = PCGJCL(student_emb_list, embd_queues, 128, 0.2 , 4, psi=4096)\n",
    "PCGJCL_loss = PCGJCL_loss.to(dev)\n",
    "print('PCGJCL_loss: ', PCGJCL_loss)\n",
    "\n",
    "model.module.contrast=False\n",
    "#calculate supervied loss\n",
    "imgs, multi_channel_masks =imgs.to(dev), multi_channel_masks.to(dev)\n",
    "out = model(imgs)\n",
    "print('masks shape: ', multi_channel_masks.shape)\n",
    "print('output shape: ', out.shape)\n",
    "\n",
    "supervised_loss = cross_entropy_loss(out,multi_channel_masks)\n",
    "supervised_loss = supervised_loss.to(dev)\n",
    "\n",
    "print('supervised_loss: ', supervised_loss)\n",
    "\n",
    "#total loss\n",
    "loss = supervised_loss + 0.5*PCGJCL_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95f77fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_epochs in range(100): #100 epochs supervised pre training\n",
    "    step=0\n",
    "    min_loss = math.inf\n",
    "    epoch_loss=0\n",
    "    print('Epoch ',c_epochs)\n",
    "    \n",
    "    total_supervised_loss = 0\n",
    "    total_contrastive_loss = 0 \n",
    "    \n",
    "    for imgs, masks in labeled_loader:\n",
    "        print(\"masks shape: \", masks.shape)\n",
    "        # 創建一個新的形狀為 [8, 2, 256, 256] 的張量\n",
    "        multi_channel_masks = torch.zeros((masks.size(0), 2, masks.size(1), masks.size(2)), dtype=torch.float32)\n",
    "        # 將單通道遮罩轉換為多通道遮罩\n",
    "        multi_channel_masks[:, 0, :, :] = masks.float()  # 原始遮罩\n",
    "        multi_channel_masks[:, 1, :, :] = 1 - masks.float()  # 反轉遮罩\n",
    "\n",
    "        t1=time.time()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            #Send psudo masks & imgs to cpu\n",
    "            p_masks = multi_channel_masks\n",
    "            imgs = imgs\n",
    "\n",
    "            #get classwise patch list\n",
    "            patch_list = _get_patches(\n",
    "                imgs, p_masks,\n",
    "                classes=num_classes,\n",
    "                background=True,\n",
    "                img_size=img_size,\n",
    "                patch_size=contrastive_batch_size\n",
    "            )\n",
    "\n",
    "            #stochastic approximation filtering and threshold update\n",
    "            #qualified_patch_list = stochastic_approx.update(patch_list)\n",
    "            qualified_patch_list = patch_list\n",
    "\n",
    "            #make augmentations for teacher model\n",
    "            augmented_patch_list = batch_augment(qualified_patch_list,contrastive_batch_size)\n",
    "\n",
    "\n",
    "            #convert to tensor\n",
    "            aug_tensor_patch_list=[]\n",
    "            qualified_tensor_patch_list=[]\n",
    "            for i in range(len(augmented_patch_list)):\n",
    "                if augmented_patch_list[i] is not None:\n",
    "                    aug_tensor_patch_list.append(torch.tensor(augmented_patch_list[i]))\n",
    "                    qualified_tensor_patch_list.append(torch.tensor(qualified_patch_list[i]))\n",
    "                else:\n",
    "                    aug_tensor_patch_list.append(None)\n",
    "                    qualified_tensor_patch_list.append(None)\n",
    "\n",
    "\n",
    "        #get embeddings of qualified patches through student model\n",
    "        model=model.train()\n",
    "        model.module.contrast=True\n",
    "        student_emb_list = get_embeddings(model,qualified_tensor_patch_list,True)\n",
    "\n",
    "        #get embeddings of augmented patches through teacher model\n",
    "        teacher_model.train()\n",
    "        teacher_model.contrast = True\n",
    "        teacher_embedding_list = get_embeddings(teacher_model,aug_tensor_patch_list,False)\n",
    "\n",
    "        #enqueue these\n",
    "        embd_queues.enqueue(teacher_embedding_list)\n",
    "\n",
    "        #calculate PCGJCL loss\n",
    "        PCGJCL_loss = PCGJCL(student_emb_list, embd_queues, 128, 0.2 , 4, psi=4096)        \n",
    "        print('PCGJCL_loss: ', PCGJCL_loss.item())\n",
    "        \n",
    "        #calculate supervied loss\n",
    "        imgs, multi_channel_masks =imgs.to(dev), multi_channel_masks.to(dev)\n",
    "        model.module.contrast=False\n",
    "        out = model(imgs)\n",
    "        \n",
    "        supervised_loss = cross_entropy_loss(out,multi_channel_masks)\n",
    "        print('supervised_loss: ', supervised_loss.item())\n",
    "\n",
    "        #total loss\n",
    "        PCGJCL_loss = PCGJCL_loss.to(dev)\n",
    "        loss = supervised_loss + 0.5*PCGJCL_loss\n",
    "        \n",
    "        total_contrastive_loss += PCGJCL_loss.item()\n",
    "        total_supervised_loss += supervised_loss.item()\n",
    "        epoch_loss+=loss.item()\n",
    "\n",
    "        #backpropagate\n",
    "        loss.backward()\n",
    "        optimizer_pretrain.step()\n",
    "\n",
    "        for param_stud, param_teach in zip(model.parameters(),teacher_model.parameters()):\n",
    "            param_teach.data.copy_(0.001*param_stud + 0.999*param_teach)\n",
    "\n",
    "        #Extras\n",
    "        t2=time.time()\n",
    "        print('step ', step, 'loss: ',loss, ' & time: ',t2-t1)\n",
    "        step+=1\n",
    "        \n",
    "    avg_epoch_loss = epoch_loss / len(labeled_loader)\n",
    "    avg_supervised_loss = total_supervised_loss / len(labeled_loader)\n",
    "    avg_contrastive_loss = total_contrastive_loss / len(labeled_loader)\n",
    "    \n",
    "    save_loss(total_loss = f\"{avg_epoch_loss:.4f}\", \n",
    "          supervised_loss=f\"{avg_supervised_loss:.4f}\", \n",
    "          contrastive_loss=f\"{avg_contrastive_loss:.4f}\", \n",
    "          consistency_loss = 0 ,\n",
    "          filename=supervised_loss_path)\n",
    "    if epoch_loss < min_loss:\n",
    "        torch.save(model,'./best_contrast.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb99729",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_epochs in range(200): #200 epochs supervised SSL\n",
    "    step=0\n",
    "    min_loss = math.inf\n",
    "    epoch_loss=0\n",
    "    print('Epoch ',c_epochs)\n",
    "    \n",
    "    total_supervised_loss = 0\n",
    "    total_contrastive_loss = 0\n",
    "    total_consistency_loss = 0\n",
    "\n",
    "    labeled_iterator = iter(labeled_loader)\n",
    "    \n",
    "    for imgs in unlabeled_loader:\n",
    "\n",
    "        t1=time.time()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            #send imgs to dev\n",
    "            imgs = imgs.to(dev)\n",
    "\n",
    "            #set model in Eval mode\n",
    "            model = model.eval()\n",
    "\n",
    "            #Get pseudo masks\n",
    "            model.module.contrast=False\n",
    "            p_masks = model(imgs)\n",
    "\n",
    "            #Send psudo masks & imgs to cpu\n",
    "            p_masks=masks\n",
    "            p_masks = p_masks.to('cpu').detach()\n",
    "            imgs = imgs.to('cpu').detach()\n",
    "\n",
    "            #Since we use labeled data for PCGJCL as well\n",
    "            imgs2, masks2 = next(labeled_iterator)\n",
    "\n",
    "            #concatenating unlabeled and labeled sets\n",
    "            p_masks = torch.cat([p_masks,masks2],dim=0)\n",
    "            imgs = torch.cat([imgs,imgs2],dim=0)\n",
    "            \n",
    "            multi_channel_masks = torch.zeros((p_masks.size(0), 2, p_masks.size(1), p_masks.size(2)), dtype=torch.float32)\n",
    "            # 將單通道遮罩轉換為多通道遮罩\n",
    "            multi_channel_masks[:, 0, :, :] = p_masks.float()  # 原始遮罩\n",
    "            multi_channel_masks[:, 1, :, :] = 1 - p_masks.float()  # 反轉遮罩\n",
    "\n",
    "            #get classwise patch list\n",
    "            patch_list = _get_patches(\n",
    "                imgs, multi_channel_masks,\n",
    "                classes=num_classes,\n",
    "                background=True,\n",
    "                img_size=img_size,\n",
    "                patch_size=contrastive_batch_size\n",
    "            )\n",
    "\n",
    "            #stochastic approximation filtering and threshold update\n",
    "            qualified_patch_list = stochastic_approx.update(patch_list)\n",
    "\n",
    "\n",
    "            #make augmentations for teacher model\n",
    "            augmented_patch_list = batch_augment(qualified_patch_list,contrastive_batch_size)\n",
    "\n",
    "            #convert to tensor\n",
    "            aug_tensor_patch_list=[]\n",
    "            qualified_tensor_patch_list=[]\n",
    "            for i in range(len(augmented_patch_list)):\n",
    "                if augmented_patch_list[i] is not None:\n",
    "                    aug_tensor_patch_list.append(torch.tensor(augmented_patch_list[i]))\n",
    "                    qualified_tensor_patch_list.append(torch.tensor(qualified_patch_list[i]))\n",
    "                else:\n",
    "                    aug_tensor_patch_list.append(None)\n",
    "                    qualified_tensor_patch_list.append(None)\n",
    "\n",
    "\n",
    "        #get embeddings of qualified patches through student model\n",
    "        model=model.train()\n",
    "        model.module.contrast=True\n",
    "        student_emb_list = get_embeddings(model,qualified_tensor_patch_list,True)\n",
    "\n",
    "        #get embeddings of augmented patches through teacher model\n",
    "        teacher_model.train()\n",
    "        teacher_model.module.contrast = True\n",
    "        teacher_embedding_list = get_embeddings(teacher_model,aug_tensor_patch_list,False)\n",
    "\n",
    "        #enqueue these\n",
    "        embd_queues.enqueue(teacher_embedding_list)\n",
    "        PCGJCL_loss = PCGJCL(student_emb_list, embd_queues, 128, 0.2 , 4, psi=4096)     \n",
    "\n",
    "        #calculate supervied loss\n",
    "        imgs2, multi_channel_masks =imgs2.to(dev), multi_channel_masks.to(dev)\n",
    "        \n",
    "        model.module.contrast = False\n",
    "        out = model(imgs)\n",
    "        supervised_loss = cross_entropy_loss(out,multi_channel_masks)\n",
    "        \n",
    "        teacher_model.module.contrast = False\n",
    "        #Consistency Loss\n",
    "        consistency_loss=consistency_cost(model,teacher_model,imgs,multi_channel_masks)\n",
    "        \n",
    "        supervised_loss = supervised_loss.to(dev)\n",
    "        PCGJCL_loss = PCGJCL_loss.to(dev)\n",
    "        consistency_loss = consistency_loss.to(dev)\n",
    "        \n",
    "        total_supervised_loss += supervised_loss.item()\n",
    "        total_contrastive_loss += PCGJCL_loss.item()\n",
    "        total_consistency_loss += consistency_loss.item() \n",
    "        \n",
    "        print(\"supervised_loss: \", supervised_loss.item())\n",
    "        print(\"PCGJCL_loss: \", PCGJCL_loss.item())\n",
    "        print(\"consistency_loss: \", consistency_loss.item())\n",
    "        \n",
    "        #total loss\n",
    "        loss = supervised_loss + 0.5*PCGJCL_loss + 4*consistency_loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        #backpropagate\n",
    "        loss.backward()\n",
    "        optimizer_ssl.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        for param_stud, param_teach in zip(model.parameters(),teacher_model.parameters()):\n",
    "            param_teach.data.copy_(0.001*param_stud + 0.999*param_teach)\n",
    "\n",
    "        #Extras\n",
    "        t2=time.time()\n",
    "        print('step ', step, 'loss: ',loss, ' & time: ',t2-t1)\n",
    "        step+=1\n",
    "        \n",
    "    avg_epoch_loss = epoch_loss / len(unlabeled_loader)\n",
    "    avg_supervised_loss = total_supervised_loss / len(unlabeled_loader)\n",
    "    avg_contrastive_loss = total_contrastive_loss / len(unlabeled_loader)\n",
    "    avg_consistency_loss = total_consistency_loss / len(unlabeled_loader)\n",
    "\n",
    "    save_loss(total_loss= f\"{avg_epoch_loss:.4f}\", \n",
    "              supervised_loss=f\"{avg_supervised_loss:.4f}\", \n",
    "              contrastive_loss=f\"{avg_contrastive_loss:.4f}\", \n",
    "              consistency_loss=f\"{avg_consistency_loss:.4f}\",\n",
    "              filename=SSL_loss_path)\n",
    "    \n",
    "    if epoch_loss < min_loss:\n",
    "        torch.save(model,'./best_contrast.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b51a12a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
